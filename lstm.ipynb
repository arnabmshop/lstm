{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMGruh4l1ls6",
        "outputId": "c17e6018-55e5-4048-ad49-fd34bcce887b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Hamlet text\n",
        "#text = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "text = gutenberg.raw('austen-sense.txt')[:10000]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data processing\n",
        "# Basic cleanup \\r character takes the cursor to the beginning of the line, not to the next line.\n",
        "text = text.lower().replace('\\n', ' ').replace('\\r', ' ')\n",
        "tokenizer = Tokenizer()\n",
        "#Neural Networks work on Numbers not words. Hence, each word in a sentence is assigned an index number.\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"Total unique words:\", total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL_bu_LCd-UT",
        "outputId": "4d62d8e5-b06b-416e-d8cb-71a2e176a309"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "# say the sentence was \"to be or not to be\". tokenizer.fit_to_texts() gave the word index dictionary {to:1,be:2,or:3,not:4}.\n",
        "# Now, Tokenizer.texts_to_sequences will give a list of the index for the text [1,2,3,4,1,2]\n",
        "token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Create sequences like [w1, w2, w3], ... At least 3 words should be there so that there are minimum 2 words to predict the third.\n",
        "for i in range(2, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "3H4mRx76eyaQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to have same length\n",
        "max_seq_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "#Input_sequence has the datatype array here."
      ],
      "metadata": {
        "id": "4ohQaiZmh_aK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into input (X) and output (label y)\n",
        "X = input_sequences[:, :-1] #All rows and all columns except the last column\n",
        "labels = input_sequences[:, -1] #Only the last column\n",
        "y = to_categorical(labels, num_classes=total_words) #Convert the words into classes."
      ],
      "metadata": {
        "id": "QnBZC4sSiB8G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "YjGt5KV7iSTm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=max_seq_len-1),\n",
        "    tf.keras.layers.LSTM(150),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ideYPE9iDff",
        "outputId": "0e975f12-a81b-4d5a-9272-830488200a08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define EarlyStopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',       # Watch validation loss\n",
        "    patience=5,               # Wait 5 epochs for improvement\n",
        "    restore_best_weights=True  # Restore weights from the best epoch\n",
        ")"
      ],
      "metadata": {
        "id": "2xUgMH8AlQNc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIj4FRHCiGtV",
        "outputId": "11e96b30-86f8-407c-9e86-cb175a9705e5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.0230 - loss: 6.2249 - val_accuracy: 0.0343 - val_loss: 5.9445\n",
            "Epoch 2/10\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.0352 - loss: 5.4905 - val_accuracy: 0.0371 - val_loss: 6.1372\n",
            "Epoch 3/10\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.0450 - loss: 5.3628 - val_accuracy: 0.0343 - val_loss: 6.2370\n",
            "Epoch 4/10\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - accuracy: 0.0462 - loss: 5.3387 - val_accuracy: 0.0400 - val_loss: 6.2569\n",
            "Epoch 5/10\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - accuracy: 0.0644 - loss: 5.2131 - val_accuracy: 0.0343 - val_loss: 6.1710\n",
            "Epoch 6/10\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.0473 - loss: 5.1126 - val_accuracy: 0.0600 - val_loss: 6.2028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('next_word_model.h5')\n",
        "# Save the tokenizer into a pickle file\n",
        "import pickle\n",
        "with open('tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li1ktGN33KB2",
        "outputId": "9f573d14-1e22-44a8-dcb3-967bd4754562"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}